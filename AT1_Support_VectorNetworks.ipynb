{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AT1_Support-VectorNetworks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DonaldTurton/UTS_ML2019_ID13304086/blob/master/AT1_Support_VectorNetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6UbeKQGdy21",
        "colab_type": "text"
      },
      "source": [
        "#Assignment 1: Understanding the Literature \"Support-Vector Networks\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bunwOGoKd6kv",
        "colab_type": "text"
      },
      "source": [
        "##Introduction:\n",
        "\n",
        "The following report is an analysis of the paper \"Support-Vector Networks\" by Corina Cortes and Vladimir Vapnik written in 1995. This report contains a brief explanation of the model proposed by the authors; an analysis of the innovation that characterized this new model; an analysis of the technical quality; some examples of the application; and constructive criticism of how the paper is presented.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDrMx9n4gRQD",
        "colab_type": "text"
      },
      "source": [
        "## Content:\n",
        "Corina Cortes and Vladimir Vapnik propose a new type of learning machine for their time, the so-called \"Support-Vector Networks\". This is a two-group classification model, which objective is to find the optimal boundary or “Hyperplane” capable of separating the training data into their respective groups without errors (or minimizing it). They also introduced the concept of the “Soft Margins” which allows the generalizability of the model and lastly, the model is tested against other learning machines. \n",
        "\n",
        "A new way to construct the decision function is developed, in which the authors demonstrate how to obtain an optimal hyperplane which minimizes the function, this optimal hyperplane is calculated by solving a quadratic programming problem. The model is based on the alteration of the order in which the function was calculated before, introducing the \"convulation of the dot product\". This enables the construction of the decision boundary in a high dimensional space, where the separation of the data becomes easier than in the original dimension space, allowing the adaptability of the model to many data distributions. \n",
        "\n",
        "As an extension to the model, the paper introduces the concept of the \"Soft Margins” concept, which enhances the generalizability of the model for the cases where is not possible to separate the training data without error, making this model as powerful as the Neural Networks. \n",
        "\n",
        "The research demonstrates the usage of this model and its performance. The model is tested and compared against human performance and different well know learning algorithms such as linear classifiers, k-nearest neighbours and Neural Networks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsP2oRslgr0s",
        "colab_type": "text"
      },
      "source": [
        "## Innovation: \n",
        "The creative idea provided by the model proposed by Cortes and Vapnik is the introduction of the Support Vector Networks which are possible by construction of the Hyperplane by using the support vectors. The idea of taking the solution surface to a high dimensional space by the implementation of the Convolution of the Dot Product and the extension of the model with the introduction of the Soft Margins, which allows the generalizability of the model to the cases where is impossible the separation of the train set. A brief explanation below. \n",
        "\n",
        "\n",
        "This model was able to perform better than the other algorithms by mapping the input vector into a high dimensional feature space, where a OPTIMAL linear decision surface is constructed called “Optimal Hyperplane”. This decision surface maximizes the distance between the two classes, which are delimitated by margins. These margins are constructed using some points “Training Data” which are called “Support Vectors” \n",
        "\n",
        "*It's interesting to note that it still the same concept proposed by Fisher in 1936, of using a linear decision surface for pattern recognition. \n",
        "\n",
        "To be able to create these new learning machines, Cortes and Vapnik had to solve 2 issues that aroused around the concept. These solutions are the concepts that make this model so innovative. A brief explanation and the evolution of the model below. \n",
        "\n",
        "1. How to find a separating hyperplane capable of generalizing well for the rest of the data?  \n",
        "2. How to treat a space with so many dimensions?\n",
        "\n",
        "The generalizability of the hyperplane was solved by Vapnik in 1965. He proposed taking a small amount of data for training and finding a hyperplane capable of separate the observations without error, Vapnik called this training data “Support Vectors\".  The expected probability of error for the test sample can be calculated using the number of training vectors and expected value of support vectors (Figure1).  This fact is what enables the ability to generalize the separation. The model states that if it's possible to construct a Hyperplane with relatively small amount of SUPPORT VECTORS (compared to the training set), then it is possible to generalize the separation.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LiIT8KeoALh",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        ">  Figure 1. Expected Probability of error in Test Sample (Cortes & Vapnik 1995).\n",
        "\n",
        "<img src=\"https://github.com/DonaldTurton/UTS_ML2019_ID13304086/blob/master/proberror.png?raw=TRUE\" width=\"500\"/>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr-oe3JsoKLN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The problem of the dimensions was solved by changing the order in which the model makes the decision. The proposed model first compares two vectors in the input space which the result is then transformed (nonlinearly) to determine the output. This characteristic is what enables the model to construct many different types of classification boundary, with the ability to adapt to the data set by taking a polynomial shape. The paper provides a good visual explanation of this concept (Figure 2).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzlwNdt1oPGg",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> Figure 2. Process of classification with support vectors (Cortes & Vapnik 1995).\n",
        "\n",
        "<img src=\"https://github.com/DonaldTurton/UTS_ML2019_ID13304086/blob/master/supportvector.png?raw=TRUE\" width=\"500\"/>\n",
        "\n",
        "\n",
        "One of the most innovative characteristics of the proposed model, it’s the way in which the mode can perform even in the scenario where it’s impossible to separate the training vectors without error. To make this possible, the concept of the SOFT MARGIN HYPERPLANE was introduced. The idea is to find a Hyperplane that allows having errors in the separation but minimizes these errors as much as possible. This extension is the characteristic that enables the model to perform in any data set which makes the model to be considered a new learning algorithm and to perform as good as the Neural Networks.\n",
        "\n",
        "Corina Cortes and Vladimir Vapnik test their propose model by comparing the results with previous work done by Leon Bottou. Bottou’s research consisted of handwriting digit recognition, where the performance of several different classifier algorithms are compared (Bottou et al. 1994).\n",
        "\n",
        "\n",
        "The \"Support-Vector Network\" was set to perform the same task, with the same data sets as the models compared by Bottou. Cortes and Vapnik were able to demonstrate how these Support-Vector Networks are able to perform better than a more complex model, such as the \"LeNet 4\", a Neural Network in Bottou benchmark, which was considered the most powerful learning algorithm at that time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no1meqn_hCtc",
        "colab_type": "text"
      },
      "source": [
        "## Technical Quality:\n",
        "\n",
        "The research seems to have adequate implementation and testing. The Support-Vector Network stated in this paper is tested using the same data sets as the models used for benchmark in Bottou’s research paper. The databases used for testing the models are US postal services Database and NIST database.\n",
        "\n",
        "The US Postal Service data set was used to measure the performance of polynomials of different degrees (1 to 7), and then compared the performance of different classification \"machines\" like Humans, Decision Tree CART, Decision Tree C4.5, 2-Layer Neural Network and 5-Layer Network \"LeNet1\". The authors clarify the complexity of the data to be solved by the Support-Vector Network because of the impossibility of separate the training data.\n",
        "\n",
        "Some irregularities were detected in the way in which the results of the models are compared. The parameter used to compare the performance is the \"Raw error %\", which seems to represent the number of misclassifications for each model. However, the paper is not clear about which errors are being compared.  The errors provided from the research performed by Bottou does not include a specification about their precedence (Train or Test set). If we look at the results provided in the original publication, LeNet1 TEST error is 1.7 and the error provided in for the comparison is 5.1(LeCun et al. 1995). This evidence suggests the error shown in Cortes and Vapnik research is obtained from the performance of the models in the Train Set, which is being compared with the error reported for the polynomial classifiers, clearly stated to be from the train set.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCe7LQJPniF8",
        "colab_type": "text"
      },
      "source": [
        "*Note: Yann LeCun 1995 paper was used to get the figures since the original Bottou Paper found did not contain the proper figures.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRjn9lIinjKX",
        "colab_type": "text"
      },
      "source": [
        ">Figure 3. Error rate on test set (LeCun et al. 1995).\n",
        "\n",
        "<img src=\"https://github.com/DonaldTurton/UTS_ML2019_ID13304086/blob/master/error_rate_LeCun.png?raw=TRUE\" width=\"500\"/>"
      ]
    }
  ]
}