{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AT1_Support-VectorNetworks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DonaldTurton/UTS_ML2019_ID13304086/blob/master/AT1_Support_VectorNetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6UbeKQGdy21",
        "colab_type": "text"
      },
      "source": [
        "#Assignment 1: Understanding the Literature \"Support-Vector Networks\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bunwOGoKd6kv",
        "colab_type": "text"
      },
      "source": [
        "##Introduction:\n",
        "\n",
        "The following report is an analysis of the paper \"Support-Vector Networks\" by Corina Cortes and Vladimir Vapnik written in 1995. This report contains a brief explanation of the model proposed by the authors; an analysis of the innovation that characterized this new model; an analysis of the technical quality; some examples of the application; and constructive criticism of how the paper is presented.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDrMx9n4gRQD",
        "colab_type": "text"
      },
      "source": [
        "## Content:\n",
        "Corina Cortes and Vladimir Vapnik propose a new type of learning machine for their time, the so-called \"Support-Vector Networks\". This is a two-group classification model, which objective is to find the optimal boundary or “Hyperplane” capable of separating the training data into their respective groups without errors (or minimizing it). They also introduced the concept of the “Soft Margins” which allows the generalizability of the model and lastly, the model is tested against other learning machines. \n",
        "\n",
        "A new way to construct the decision function is developed, in which the authors demonstrate how to obtain an optimal hyperplane which minimizes the function, this optimal hyperplane is calculated by solving a quadratic programming problem. The model is based on the alteration of the order in which the function was calculated before, introducing the \"convulation of the dot product\". This enables the construction of the decision boundary in a high dimensional space, where the separation of the data becomes easier than in the original dimension space, allowing the adaptability of the model to many data distributions. \n",
        "\n",
        "As an extension to the model, the paper introduces the concept of the \"Soft Margins” concept, which enhances the generalizability of the model for the cases where is not possible to separate the training data without error, making this model as powerful as the Neural Networks. \n",
        "\n",
        "The research demonstrates the usage of this model and its performance. The model is tested and compared against human performance and different well know learning algorithms such as linear classifiers, k-nearest neighbours and Neural Networks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsP2oRslgr0s",
        "colab_type": "text"
      },
      "source": [
        "## Innovation: \n",
        "The creative idea provided by the model proposed by Cortes and Vapnik is the introduction of the Support Vector Networks which are possible by construction of the Hyperplane by using the support vectors. The idea of taking the solution surface to a high dimensional space by the implementation of the Convolution of the Dot Product and the extension of the model with the introduction of the Soft Margins, which allows the generalizability of the model to the cases where is impossible the separation of the train set. A brief explanation below. \n",
        "\n",
        "\n",
        "This model was able to perform better than the other algorithms by mapping the input vector into a high dimensional feature space, where a OPTIMAL linear decision surface is constructed called “Optimal Hyperplane”. This decision surface maximizes the distance between the two classes, which are delimitated by margins. These margins are constructed using some points “Training Data” which are called “Support Vectors” \n",
        "\n",
        "*It's interesting to note that it still the same concept proposed by Fisher in 1936, of using a linear decision surface for pattern recognition. \n",
        "\n",
        "To be able to create these new learning machines, Cortes and Vapnik had to solve 2 issues that aroused around the concept. These solutions are the concepts that make this model so innovative. A brief explanation and the evolution of the model below. \n",
        "\n",
        "1. How to find a separating hyperplane capable of generalizing well for the rest of the data?  \n",
        "2. How to treat a space with so many dimensions?\n",
        "\n",
        "The generalizability of the hyperplane was solved by Vapnik in 1965. He proposed taking a small amount of data for training and finding a hyperplane capable of separate the observations without error, Vapnik called this training data “Support Vectors\".  The expected probability of error for the test sample can be calculated using the number of training vectors and expected value of support vectors (Figure1).  This fact is what enables the ability to generalize the separation. The model states that if it's possible to construct a Hyperplane with relatively small amount of SUPPORT VECTORS (compared to the training set), then it is possible to generalize the separation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ">  Figure 1. Expected Probability of error in Test Sample (Cortes & Vapnik 1995).\n",
        "\n",
        "![alt text](https://github.com/DonaldTurton/UTS_ML2019_ID13304086/blob/master/proberror.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no1meqn_hCtc",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}